{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aab9e70-7dec-4acc-9d47-5d80aca06344",
   "metadata": {},
   "source": [
    "# Learned Decoder\n",
    "The purpose of this notebook is to investigate whether we can learn the MURA decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5283d75d-8a66-4b82-92b2-5c1ae2ac60d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a98262-0eb9-4894-a91b-6c1ac2c0b165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path =  /Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "# import sklearn\n",
    "import wandb\n",
    "\n",
    "desktop_path = '/Users/jornelasmunoz/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/'\n",
    "laptop_path = '/Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/'\n",
    "if desktop_path in sys.path[0]: sys.path.insert(0, desktop_path + 'lib/'); path = desktop_path\n",
    "elif laptop_path in sys.path[0]: sys.path.insert(0, laptop_path + 'lib/'); path = laptop_path\n",
    "print('Using path = ', path)\n",
    "\n",
    "\n",
    "\n",
    "# # Change plotting parameters\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times\", #\"Computer Modern Serif\"\n",
    "#     \"figure.figsize\" : [15,10],\n",
    "# })\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Log into WandB\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910e3393-0989-4e46-9ca7-9ddbe08bcbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Noise_dB(object):\n",
    "#     '''\n",
    "#     Given a desired Signal to Noise Ratio (in decibels, dB)\n",
    "#     returns a noisy image\n",
    "    \n",
    "#     Inputs:\n",
    "#         desired_snr: Integer. Signal to noise ration in decibels \n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, desired_snr=10):\n",
    "#         super().__init__()\n",
    "#         self.snr = desired_snr\n",
    "\n",
    "#     def __call__(self, tensor):\n",
    "#          # Calculate the variance of the image pixels\n",
    "#         signal_power = torch.var(tensor)\n",
    "    \n",
    "#         # Calculate the noise power\n",
    "#         noise_power = signal_power / (10**(self.snr/10))\n",
    "    \n",
    "#         # Generate random noise matrix\n",
    "#         noise = torch.normal(0,torch.sqrt(noise_power), size=tensor.shape)\n",
    "    \n",
    "#         # Add the noise to the image\n",
    "#         noisy_image = tensor + noise\n",
    "#         # noisy_image = torch.clip(noisy_image, 0, 1)\n",
    "\n",
    "#         return noisy_image\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + '(snr = {0})'.format(self.snr)\n",
    "\n",
    "class Noise(object):\n",
    "    def __init__(self, mean=0, dev=1):\n",
    "        self.mean = mean\n",
    "        self.dev = dev\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size())*self.dev + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + 'mean = {0}, dev= {1}', format(self.mean, self.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c477d885-9fd0-4134-baf2-319c84c7f34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data and then do MURA transformations\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.mnist import read_image_file, read_label_file\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "import MURA as mura\n",
    "\n",
    "class MNIST_MURA(VisionDataset):\n",
    "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
    "            and  ``MNIST/processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    mirrors = [\n",
    "        'http://yann.lecun.com/exdb/mnist/',\n",
    "        'https://ossci-datasets.s3.amazonaws.com/mnist/',\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "        (\"train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "        (\"t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "        (\"t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "    ]\n",
    "\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
    "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "\n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self):\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    def __init__(self, root, params, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(MNIST_MURA, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "\n",
    "        self.train = train  # training set or test set\n",
    "        self.params = params\n",
    "        self._read_params(self.params)\n",
    "        \n",
    "        if self._check_legacy_exist():\n",
    "            self.data, self.targets = self._load_legacy_data()\n",
    "            \n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        # Load data\n",
    "        self.data, self.targets, self.digits = self._load_data()\n",
    "    \n",
    "    \n",
    "    def _check_exists(self):\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0]))\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "    def _check_legacy_exist(self):\n",
    "        processed_folder_exists = os.path.exists(self.processed_folder)\n",
    "        if not processed_folder_exists:\n",
    "            return False\n",
    "    def _load_legacy_data(self):\n",
    "        # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data\n",
    "        # directly.\n",
    "        data_file = self.training_file if self.train else self.test_file\n",
    "        return torch.load(os.path.join(self.processed_folder, data_file))\n",
    "        \n",
    "    def _load_data(self):\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        data = read_image_file(os.path.join(self.raw_folder, image_file))\n",
    "        \n",
    "        # Compute MURA encoder and decoder (numpy)\n",
    "        self.A = mura.create_binary_aperture_arr(self.image_size)\n",
    "        self.G = mura.create_decoding_arr(self.A)\n",
    "        # Resize data to prime number length and convolve with aperture\n",
    "        data_resized = torchvision.transforms.functional.resize(data, [self.image_size,self.image_size], antialias=True)\n",
    "        mura_data = torch.empty(data_resized.size())\n",
    "        for idx, img in enumerate(data_resized):\n",
    "            mura_data[idx] = torch.tensor(\n",
    "                            mura.normalize(mura.FFT_convolve(\n",
    "                            np.squeeze(img.numpy()), self.A,self.image_size)), dtype= torch.float)\n",
    "            \n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        digits = read_label_file(os.path.join(self.raw_folder, label_file))\n",
    "        \n",
    "        \n",
    "        # #Technically, we do not even need the labels for now\n",
    "        # # We just need the clean images of both types\n",
    "        # randata = data[torch.randperm(data.shape[0]),:,:]\n",
    "        # targets = (data, randata)\n",
    "        \n",
    "        # # Now do the ambiguation here\n",
    "        # data = data + randata\n",
    "        \n",
    "        return mura_data, data_resized, digits\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target, digit = self.data[index], self.targets[index], self.digits[index]\n",
    "        #Change img to numpy and range to [0,155]\n",
    "        img = np.uint8((img*255).numpy())\n",
    "        \n",
    "        #doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Imagedata[torch.randperm(data.shape[0]),:,:]\n",
    "        img = Image.fromarray(img, mode='L')\n",
    "        target = Image.fromarray(target.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)            \n",
    "\n",
    "        return dict({'img': img, 'target': target, 'digit': digit.item()})\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "        \n",
    "    def _read_params(self, params):\n",
    "        self.image_size = self.params['image_size']\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self):\n",
    "        return os.path.join(self.root, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self):\n",
    "        return os.path.join(self.root, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f1e161-55f9-44fe-bf66-0f6ad3bd75e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjocelynornelasmunoz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/notebooks/wandb/run-20231020_171155-dt1gg01o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/dt1gg01o' target=\"_blank\">glowing-cloud-1</a></strong> to <a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder' target=\"_blank\">https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/dt1gg01o' target=\"_blank\">https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/dt1gg01o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/dt1gg01o?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1261ddad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log into WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c063d5-6039-4ab2-a865-b7ae6655c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create params dict\n",
    "params = {\n",
    "  \"image_size\": 23,\n",
    "  \"batch_size\":100,\n",
    "  \"learning_rate\": 0.001, \n",
    "  \"epochs\": 100,\n",
    "  \"model\": \"decoder_cnn\"\n",
    "}\n",
    "wandb.config = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13e1b7a-0a08-4db8-8c84-e340fa4081b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform_list = transforms.Compose(\n",
    "            [   transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(params['image_size'], antialias=True),\n",
    "                # Noise_dB(desired_snr=1),\n",
    "                # transforms.Normalize(0, 1),\n",
    "                ])\n",
    "target_transform = transforms.Compose(\n",
    "            [   transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((params['image_size'],params['image_size']), antialias=True),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6e757d-5b6e-427b-a406-715d55290b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mnist_mura = MNIST_MURA('../data/MNIST/', params, transform=transform_list, target_transform=target_transform, train=True)\n",
    "\n",
    "# Define DataLoader\n",
    "trainset = DataLoader(mnist_mura, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900cc389-487e-4fab-a03e-847a46dbf3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,7))\n",
    "idx=1\n",
    "axs[0].imshow(trainset.dataset[idx]['target'].squeeze(0), cmap= 'gray')\n",
    "axs[1].imshow(trainset.dataset[idx]['img'].squeeze(0), cmap= 'gray')\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af062d-09da-4c93-9694-c05e6e4c0032",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6ba0e00-43b7-4fcc-9b1a-2e2d9c3201b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RECON_CNN(torch.nn.Module):\n",
    "    '''\n",
    "    Define a model with only one convolutional layer and sigmoid activation function\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super().__init__() \n",
    "        \n",
    "        # Define model basic info\n",
    "        self.params = params\n",
    "        self.img_size = self.params['image_size']\n",
    "        self.kernel_size = self.params['image_size'] if self.params['image_size'] is not None else self.params['kernel_size']\n",
    "        self.params[\"kernel_size\"] = self.kernel_size\n",
    "        self.criterion = torch.nn.MSELoss() if self.params.get('loss') is None else torch.nn.L1Loss() #\n",
    "        self.params['model_save_path'] = f'../models/learned_decoder_cnn/{params[\"model\"]}.pth'\n",
    "        \n",
    "        # Define model architecture elements\n",
    "        self.conv  = torch.nn.Conv2d(1,1,kernel_size=self.kernel_size, padding=(self.kernel_size-1)//2)\n",
    "        #self.convT = torch.nn.ConvTranspose2d(1,1,kernel_size=self.kernel_size,padding=(self.kernel_size-1)//2) \n",
    "        print(\"Using the following parameters:\")\n",
    "        for key, val in self.params.items():\n",
    "            print(f\"{key}: {val}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if \"activation\" in self.params['model']:\n",
    "            output = torch.sigmoid(self.conv(x))\n",
    "        else:\n",
    "            # 03.20.23 Trying out a model with no activation function -- update 03.28.23 Didnt work if it's just no activation :( \n",
    "            # 04.12.23 Trying no activation function with l_1 penalty\n",
    "            #output = self.conv(x)\n",
    "            \n",
    "            # Trying convolution transpose as test\n",
    "            output = self.conv(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "158ff68d-be2d-4f2b-a3d6-7ebbac1975f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following parameters:\n",
      "image_size: 23\n",
      "batch_size: 100\n",
      "learning_rate: 0.001\n",
      "epochs: 100\n",
      "model: decoder_cnn\n",
      "model_save_path: ../models/learned_decoder_cnn/decoder_cnn.pth\n",
      "kernel_size: 23\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model \n",
    "model = RECON_CNN(params)#.to(device)\n",
    "model.optimizer = torch.optim.Adam(model.parameters(), lr = model.params['learning_rate']) #torch.optim.SGD(model.parameters(), lr = model.params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0b301ed-2992-4e2a-9a45-a134edbdfab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-20 18:12:11.947075\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "print(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c876fc9-e3d7-4489-a0b8-d06922820f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize project in Weights and Biases\n",
    "wandb.init(config=wandb.config, \n",
    "           project=\"ca-learned-decoder\", \n",
    "           group=\"cnn\", \n",
    "           name=str(datetime.datetime.now()))\n",
    "\n",
    "# Store values for later \n",
    "train_loss = []\n",
    "# val_loss = []\n",
    "frob_per_epoch = []\n",
    "\n",
    "#Dictionary that will store different images and outputs for various epochs (not sure if needed)\n",
    "outputs = {}\n",
    "\n",
    "# Training loop starts\n",
    "for epoch in range(params['epochs']):\n",
    "    \n",
    "    # Initialize variable to store loss\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    # Iterate over training set\n",
    "    for i, data in enumerate(loaders['train']):\n",
    "        # get the inputs; data is a list of [images, labels, digit]\n",
    "        inputs, targets, digits = data.values()\n",
    "        else: \n",
    "            # data is a list of [images, labels, digit, noise level]\n",
    "            inputs, targets, digits,_ = data\n",
    "            \n",
    "        # Generate output\n",
    "        out = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = model.criterion(out, targets)\n",
    "        \n",
    "        # # Modified (04/06/23) to include l_1 penalty\n",
    "        # l_1_pen = sum((w.abs()-1).abs().sum() for w in model.parameters())\n",
    "        # loss = loss + (model.params['lambda']/(model.params['p']**2)) * l_1_pen\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        model.optimizer.zero_grad()\n",
    "        # Backprop and update weights\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        # Increment loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Average loss over entire dataset\n",
    "    running_loss/= len(loaders['train'].dataset)#params['batch_size']\n",
    "    train_loss.append(running_loss)  \n",
    "    \n",
    "    \n",
    "    # --------------------- Validation ------------------\n",
    "    model.eval()\n",
    "    running_vloss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(loaders['eval']):\n",
    "            if len(vdata) == 3:\n",
    "                vinputs, vtargets, _ = vdata\n",
    "            else:\n",
    "                vinputs, vtargets, _,_ = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            # validation loss modified to include penalty\n",
    "            # vloss = (model.criterion(voutputs, vtargets) + (model.params['lambda']/(model.params['p']**2)) * l_1_pen).item()\n",
    "            vloss = model.criterion(voutputs, vtargets)\n",
    "            running_vloss += vloss\n",
    "        running_vloss/= len(loaders['eval'].dataset)\n",
    "        val_loss.append(running_vloss) \n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch +1} | Loss: {running_loss:.7f} | Val_loss: {running_vloss:.7f}\")\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": running_loss, \"val_loss\": running_vloss})\n",
    "    \n",
    "    # Storing images, reconstructed outputs, and labels\n",
    "    outputs[epoch+1] = {'img': inputs, 'out': out, 'targets': targets, 'digits': digits}\n",
    "    \n",
    "    # Save weights every 10 epochs\n",
    "    if epoch % 10 == 9:\n",
    "        if abs(train_loss[epoch] < train_loss[epoch-1]):\n",
    "            torch.save(model.state_dict(), model.params['model_save_path'])\n",
    "        # epoch_save_model = path+f'models/CNN/{params[\"model\"]}_model_epoch_'+str(epoch)+'.pth'\n",
    "        # torch.save(model.state_dict(),epoch_save_model)\n",
    "        weight_epoch_list = list(model.parameters())\n",
    "        weight_epoch = np.squeeze(weight_epoch_list[0].detach().cpu().numpy())\n",
    "        plt.figure(figsize=(15,15))\n",
    "        heat = sns.heatmap(weight_epoch, cmap='gray')\n",
    "        figure = heat.get_figure()\n",
    "        figure.savefig(path+f'/metrics/CNN/{params[\"model\"]}_model_heat_epoch_'+str(epoch)+'.png',dpi=400)\n",
    "        plt.close()\n",
    "    \n",
    "    # Calculate Frobenius norm between weights and decoder every epoch\n",
    "    weights = list(model.parameters())\n",
    "    weight_map = np.squeeze(weights[0].detach().cpu().numpy())\n",
    "    diff_block = params['G'] - weight_map\n",
    "    frob_per_epoch.append(np.linalg.norm(np.abs(diff_block),ord='fro'))\n",
    "\n",
    "#torch.save(model.state_dict(), PATH)\n",
    "# End WandB run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
