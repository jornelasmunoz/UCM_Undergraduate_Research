{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aab9e70-7dec-4acc-9d47-5d80aca06344",
   "metadata": {},
   "source": [
    "# Learned Decoder\n",
    "The purpose of this notebook is to investigate whether we can learn the MURA decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5283d75d-8a66-4b82-92b2-5c1ae2ac60d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a98262-0eb9-4894-a91b-6c1ac2c0b165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path =  /Users/jornelasmunoz/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "# import sklearn\n",
    "import wandb\n",
    "\n",
    "desktop_path = '/Users/jornelasmunoz/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/'\n",
    "laptop_path = '/Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/'\n",
    "if desktop_path in sys.path[0]: sys.path.insert(0, desktop_path + 'lib/'); path = desktop_path\n",
    "elif laptop_path in sys.path[0]: sys.path.insert(0, laptop_path + 'lib/'); path = laptop_path\n",
    "print('Using path = ', path)\n",
    "\n",
    "from dataset import MNIST_MURA\n",
    "\n",
    "\n",
    "# # Change plotting parameters\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times\", #\"Computer Modern Serif\"\n",
    "#     \"figure.figsize\" : [15,10],\n",
    "# })\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Log into WandB\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910e3393-0989-4e46-9ca7-9ddbe08bcbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Noise_dB(object):\n",
    "#     '''\n",
    "#     Given a desired Signal to Noise Ratio (in decibels, dB)\n",
    "#     returns a noisy image\n",
    "    \n",
    "#     Inputs:\n",
    "#         desired_snr: Integer. Signal to noise ration in decibels \n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, desired_snr=10):\n",
    "#         super().__init__()\n",
    "#         self.snr = desired_snr\n",
    "\n",
    "#     def __call__(self, tensor):\n",
    "#          # Calculate the variance of the image pixels\n",
    "#         signal_power = torch.var(tensor)\n",
    "    \n",
    "#         # Calculate the noise power\n",
    "#         noise_power = signal_power / (10**(self.snr/10))\n",
    "    \n",
    "#         # Generate random noise matrix\n",
    "#         noise = torch.normal(0,torch.sqrt(noise_power), size=tensor.shape)\n",
    "    \n",
    "#         # Add the noise to the image\n",
    "#         noisy_image = tensor + noise\n",
    "#         # noisy_image = torch.clip(noisy_image, 0, 1)\n",
    "\n",
    "#         return noisy_image\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + '(snr = {0})'.format(self.snr)\n",
    "\n",
    "class Noise(object):\n",
    "    def __init__(self, mean=0, dev=1):\n",
    "        self.mean = mean\n",
    "        self.dev = dev\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size())*self.dev + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + 'mean = {0}, dev= {1}', format(self.mean, self.dev)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f696a3d-88b4-4db3-b98d-07f622ba9f02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Load the data and then do MURA transformations\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.mnist import read_image_file, read_label_file\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "import MURA as mura\n",
    "\n",
    "class MNIST_MURA(VisionDataset):\n",
    "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
    "            and  ``MNIST/processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    mirrors = [\n",
    "        'http://yann.lecun.com/exdb/mnist/',\n",
    "        'https://ossci-datasets.s3.amazonaws.com/mnist/',\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "        (\"train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "        (\"t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "        (\"t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "    ]\n",
    "\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
    "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "\n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self):\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    def __init__(self, root, params, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(MNIST_MURA, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "\n",
    "        self.train = train  # training set or test set\n",
    "        self.params = params\n",
    "        self._read_params(self.params)\n",
    "        \n",
    "        if self._check_legacy_exist():\n",
    "            self.data, self.targets = self._load_legacy_data()\n",
    "            \n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        # Load data\n",
    "        self.data, self.targets, self.digits = self._load_data()\n",
    "    \n",
    "    \n",
    "    def _check_exists(self):\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0]))\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "    def _check_legacy_exist(self):\n",
    "        processed_folder_exists = os.path.exists(self.processed_folder)\n",
    "        if not processed_folder_exists:\n",
    "            return False\n",
    "    def _load_legacy_data(self):\n",
    "        # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data\n",
    "        # directly.\n",
    "        data_file = self.training_file if self.train else self.test_file\n",
    "        return torch.load(os.path.join(self.processed_folder, data_file))\n",
    "        \n",
    "    def _load_data(self):\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        data = read_image_file(os.path.join(self.raw_folder, image_file))\n",
    "        \n",
    "        # Compute MURA encoder and decoder (numpy)\n",
    "        self.A = mura.create_binary_aperture_arr(self.image_size)\n",
    "        self.G = mura.create_decoding_arr(self.A)\n",
    "        # Resize data to prime number length and convolve with aperture\n",
    "        data_resized = torchvision.transforms.functional.resize(data, [self.image_size,self.image_size], antialias=True)\n",
    "        mura_data = torch.empty(data_resized.size())\n",
    "        for idx, img in enumerate(data_resized):\n",
    "            mura_data[idx] = torch.tensor(\n",
    "                            mura.normalize(mura.FFT_convolve(\n",
    "                            np.squeeze(img.numpy()), self.A,self.image_size)), dtype= torch.float)\n",
    "            \n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        digits = read_label_file(os.path.join(self.raw_folder, label_file))\n",
    "        \n",
    "        \n",
    "        # #Technically, we do not even need the labels for now\n",
    "        # # We just need the clean images of both types\n",
    "        # randata = data[torch.randperm(data.shape[0]),:,:]\n",
    "        # targets = (data, randata)\n",
    "        \n",
    "        # # Now do the ambiguation here\n",
    "        # data = data + randata\n",
    "        \n",
    "        return mura_data, data_resized, digits\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target, digit = self.data[index], self.targets[index], self.digits[index]\n",
    "        #Change img to numpy and range to [0,155]\n",
    "        img = np.uint8((img*255).numpy())\n",
    "        \n",
    "        #doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Imagedata[torch.randperm(data.shape[0]),:,:]\n",
    "        img = Image.fromarray(img, mode='L')\n",
    "        target = Image.fromarray(target.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)            \n",
    "\n",
    "        return dict({'img': img, 'target': target, 'digit': digit.item()})\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "        \n",
    "    def _read_params(self, params):\n",
    "        self.image_size = self.params['image_size']\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self):\n",
    "        return os.path.join(self.root, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self):\n",
    "        return os.path.join(self.root, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f1e161-55f9-44fe-bf66-0f6ad3bd75e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjocelynornelasmunoz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log into WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c063d5-6039-4ab2-a865-b7ae6655c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create params dict\n",
    "params = {\n",
    "  \"image_size\": 23,\n",
    "  \"batch_size\":100,\n",
    "  \"learning_rate\": 0.001, \n",
    "  \"epochs\": 10,\n",
    "  \"model\": \"decoder_cnn\"\n",
    "}\n",
    "wandb.config = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13e1b7a-0a08-4db8-8c84-e340fa4081b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform_list = transforms.Compose(\n",
    "            [   transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(params['image_size'], antialias=True),\n",
    "                # Noise_dB(desired_snr=1),\n",
    "                # transforms.Normalize(0, 1),\n",
    "                ])\n",
    "target_transform = transforms.Compose(\n",
    "            [   transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((params['image_size'],params['image_size']), antialias=True),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6e757d-5b6e-427b-a406-715d55290b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mnist_mura = MNIST_MURA('../data/MNIST/', params, transform=transform_list, target_transform=target_transform, train=True)\n",
    "\n",
    "loaders = {}\n",
    "# Define DataLoader\n",
    "loaders['train'] = DataLoader(mnist_mura, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "900cc389-487e-4fab-a03e-847a46dbf3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,7))\n",
    "idx=1\n",
    "axs[0].imshow(loaders['train'].dataset[idx]['target'].squeeze(0), cmap= 'gray')\n",
    "axs[1].imshow(loaders['train'].dataset[idx]['img'].squeeze(0), cmap= 'gray')\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af062d-09da-4c93-9694-c05e6e4c0032",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ba0e00-43b7-4fcc-9b1a-2e2d9c3201b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RECON_CNN(torch.nn.Module):\n",
    "    '''\n",
    "    Define a model with only one convolutional layer and sigmoid activation function\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super().__init__() \n",
    "        \n",
    "        # Define model basic info\n",
    "        self.params = params\n",
    "        self.img_size = self.params['image_size']\n",
    "        self.kernel_size = self.params['image_size'] if self.params['image_size'] is not None else self.params['kernel_size']\n",
    "        self.params[\"kernel_size\"] = self.kernel_size\n",
    "        self.criterion = torch.nn.MSELoss() if self.params.get('loss') is None else torch.nn.L1Loss() #\n",
    "        self.params['model_save_path'] = f'../models/learned_decoder_cnn/{params[\"model\"]}.pth'\n",
    "        \n",
    "        # Define model architecture elements\n",
    "        self.conv  = torch.nn.Conv2d(1,1,kernel_size=self.kernel_size, padding=(self.kernel_size-1)//2)\n",
    "        print(\"Using the following parameters:\")\n",
    "        for key, val in self.params.items():\n",
    "            print(f\"{key}: {val}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if \"activation\" in self.params['model']:\n",
    "            output = torch.sigmoid(self.conv(x))\n",
    "        else:\n",
    "            # 03.20.23 Trying out a model with no activation function -- update 03.28.23 Didnt work if it's just no activation :( \n",
    "            # 04.12.23 Trying no activation function with l_1 penalty\n",
    "            #output = self.conv(x)\n",
    "            \n",
    "            # Trying convolution transpose as test\n",
    "            output = self.conv(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158ff68d-be2d-4f2b-a3d6-7ebbac1975f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following parameters:\n",
      "image_size: 23\n",
      "batch_size: 100\n",
      "learning_rate: 0.001\n",
      "epochs: 10\n",
      "model: decoder_cnn\n",
      "kernel_size: 23\n",
      "model_save_path: ../models/learned_decoder_cnn/decoder_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model \n",
    "model = RECON_CNN(params)#.to(device)\n",
    "model.optimizer = torch.optim.Adam(model.parameters(), lr = model.params['learning_rate']) #torch.optim.SGD(model.parameters(), lr = model.params['learning_rate'])\n",
    "model.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model.optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2369dddd-0d83-4e12-afc5-69c51d2c7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3837b26a-9547-4e56-91d6-9c5c3613d4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2300, 23])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(mnist_mura.G).repeat(model.params['batch_size'],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c876fc9-e3d7-4489-a0b8-d06922820f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jornelasmunoz/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/notebooks/wandb/run-20231027_135412-7zrsidlb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/7zrsidlb' target=\"_blank\">2023-10-27 13:54:12.756506</a></strong> to <a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder' target=\"_blank\">https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/7zrsidlb' target=\"_blank\">https://wandb.ai/jocelynornelasmunoz/ca-learned-decoder/runs/7zrsidlb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jornelasmunoz/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([23, 23])) that is different to the input size (torch.Size([1, 1, 23, 23])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:24\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m}\u001b[49m)\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime\n",
    "# Initialize project in Weights and Biases\n",
    "wandb.init(config=wandb.config, \n",
    "           project=\"ca-learned-decoder\", \n",
    "           group=\"cnn\", \n",
    "           name=str(datetime.datetime.now()))\n",
    "\n",
    "# Store values for later \n",
    "train_loss = []\n",
    "# val_loss = []\n",
    "frob_per_epoch = []\n",
    "weights_epoch = {}\n",
    "\n",
    "#Dictionary that will store different images and outputs for various epochs (not sure if needed)\n",
    "outputs = {}\n",
    "\n",
    "# Training loop starts\n",
    "for epoch in range(params['epochs']):\n",
    "    \n",
    "    # Initialize variable to store loss\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    # Iterate over training set\n",
    "    for i, data in enumerate(loaders['train']):\n",
    "        # zero the parameter gradients\n",
    "        model.optimizer.zero_grad()\n",
    "        \n",
    "        # get the inputs; data is a list of [images, labels, digit]\n",
    "        inputs, targets, digits = data['img'], data['target'], data['digit']\n",
    "            \n",
    "        # Generate output\n",
    "        out = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        # 10.27.23 changed to l1 comparison with G\n",
    "        loss = model.criterion(out, targets)\n",
    "        \n",
    "        # # Modified (04/06/23) to include l_1 penalty\n",
    "        # l_1_pen = sum((w.abs()-1).abs().sum() for w in model.parameters())\n",
    "        # loss = loss + (model.params['lambda']/(model.params['p']**2)) * l_1_pen\n",
    "\n",
    "        \n",
    "        # Backprop and update weights\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        # Increment loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Average loss over entire dataset\n",
    "    running_loss/= len(loaders['train'].dataset)#params['batch_size']\n",
    "    train_loss.append(running_loss)\n",
    "\n",
    "    # Scheduler step every epoch\n",
    "    model.scheduler.step(loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch +1} | Loss: {running_loss:.7f}\")\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": running_loss})\n",
    "    \n",
    "    # Storing images, reconstructed outputs, and labels\n",
    "    outputs[epoch+1] = {'img': inputs, 'out': out, 'targets': targets, 'digits': digits}\n",
    "    \n",
    "    # Save weights every 10 epochs\n",
    "    if epoch % 2 == 1:\n",
    "        torch.save(model.state_dict(), model.params['model_save_path'])\n",
    "        # epoch_save_model = path+f'models/CNN/{params[\"model\"]}_model_epoch_'+str(epoch)+'.pth'\n",
    "        # torch.save(model.state_dict(),epoch_save_model)\n",
    "        weight_epoch_list = list(model.parameters())\n",
    "        weights_epoch[f'{epoch}'] = np.squeeze(weight_epoch_list[0].detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    # Calculate Frobenius norm between weights and decoder every epoch\n",
    "    weights = list(model.parameters())\n",
    "    weight_map = np.squeeze(weights[0].detach().cpu().numpy())\n",
    "    diff_block = mnist_mura.G - weight_map\n",
    "    frob_per_epoch.append(np.linalg.norm(np.abs(diff_block),ord='fro'))\n",
    "\n",
    "#torch.save(model.state_dict(), PATH)\n",
    "# End WandB run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e409b-709f-4156-b0e6-6ee2dc5bedc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
