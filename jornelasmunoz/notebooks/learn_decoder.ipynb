{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aab9e70-7dec-4acc-9d47-5d80aca06344",
   "metadata": {},
   "source": [
    "# Learned Decoder\n",
    "The purpose of this notebook is to investigate whether we can learn the MURA decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5283d75d-8a66-4b82-92b2-5c1ae2ac60d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a98262-0eb9-4894-a91b-6c1ac2c0b165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using path =  /Users/jornelasmunoz/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchsummary import summary\n",
    "# import sklearn\n",
    "# import wandb\n",
    "\n",
    "desktop_path = '/Users/jornelasmunoz/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/'\n",
    "laptop_path = '/Users/jocelynornelas/iCloud Drive (Archive)/Desktop/UC Merced/Research/coded-aperture/jornelasmunoz/'\n",
    "if desktop_path in sys.path[0]: sys.path.insert(0, desktop_path + 'lib/'); path = desktop_path\n",
    "elif laptop_path in sys.path[0]: sys.path.insert(0, laptop_path + 'lib/'); path = laptop_path\n",
    "print('Using path = ', path)\n",
    "\n",
    "\n",
    "\n",
    "# # Change plotting parameters\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"Times\", #\"Computer Modern Serif\"\n",
    "#     \"figure.figsize\" : [15,10],\n",
    "# })\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Log into WandB\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910e3393-0989-4e46-9ca7-9ddbe08bcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise_dB(object):\n",
    "    '''\n",
    "    Given a desired Signal to Noise Ratio (in decibels, dB)\n",
    "    returns a noisy image\n",
    "    \n",
    "    Inputs:\n",
    "        desired_snr: Integer. Signal to noise ration in decibels \n",
    "    '''\n",
    "\n",
    "    def __init__(self, desired_snr=10):\n",
    "        super().__init__()\n",
    "        self.snr = desired_snr\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "         # Calculate the variance of the image pixels\n",
    "        signal_power = torch.var(tensor)\n",
    "    \n",
    "        # Calculate the noise power\n",
    "        noise_power = signal_power / (10**(self.snr/10))\n",
    "    \n",
    "        # Generate random noise matrix\n",
    "        noise = torch.normal(0,torch.sqrt(noise_power), size=tensor.shape)\n",
    "    \n",
    "        # Add the noise to the image\n",
    "        noisy_image = tensor + noise\n",
    "        # noisy_image = torch.clip(noisy_image, 0, 1)\n",
    "\n",
    "        return noisy_image\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(snr = {0})'.format(self.snr)\n",
    "\n",
    "# class Noise(object):\n",
    "#     def __init__(self, mean=0, dev=1):\n",
    "#         self.mean = mean\n",
    "#         self.dev = dev\n",
    "    \n",
    "#     def __call__(self, tensor):\n",
    "#         return tensor + torch.randn(tensor.size())*self.dev + self.mean\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + 'mean = {0}, dev= {1}', format(self.mean, self.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c477d885-9fd0-4134-baf2-319c84c7f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and then do MURA transformations\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.mnist import read_image_file, read_label_file\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "import MURA as mura\n",
    "\n",
    "class MNIST_MURA(VisionDataset):\n",
    "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
    "            and  ``MNIST/processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    mirrors = [\n",
    "        'http://yann.lecun.com/exdb/mnist/',\n",
    "        'https://ossci-datasets.s3.amazonaws.com/mnist/',\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"train-images-idx3-ubyte.gz\", \"f68b3c2dcbeaaa9fbdd348bbdeb94873\"),\n",
    "        (\"train-labels-idx1-ubyte.gz\", \"d53e105ee54ea40749a09fcbcd1e9432\"),\n",
    "        (\"t10k-images-idx3-ubyte.gz\", \"9fb629c4189551a2d022fa330f9573f3\"),\n",
    "        (\"t10k-labels-idx1-ubyte.gz\", \"ec29112dd5afa0611ce80d1b7f02629c\")\n",
    "    ]\n",
    "\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four',\n",
    "               '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "\n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        warnings.warn(\"train_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self):\n",
    "        warnings.warn(\"test_labels has been renamed targets\")\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def train_data(self):\n",
    "        warnings.warn(\"train_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    @property\n",
    "    def test_data(self):\n",
    "        warnings.warn(\"test_data has been renamed data\")\n",
    "        return self.data\n",
    "\n",
    "    def __init__(self, root, params, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(MNIST_MURA, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "\n",
    "        self.train = train  # training set or test set\n",
    "        self.params = params\n",
    "        self._read_params(self.params)\n",
    "        \n",
    "        if self._check_legacy_exist():\n",
    "            self.data, self.targets = self._load_legacy_data()\n",
    "            \n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        # Load data\n",
    "        self.data, self.targets, self.digits = self._load_data()\n",
    "    \n",
    "    \n",
    "    def _check_exists(self):\n",
    "        return all(\n",
    "            check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0]))\n",
    "            for url, _ in self.resources\n",
    "        )\n",
    "    def _check_legacy_exist(self):\n",
    "        processed_folder_exists = os.path.exists(self.processed_folder)\n",
    "        if not processed_folder_exists:\n",
    "            return False\n",
    "    def _load_legacy_data(self):\n",
    "        # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data\n",
    "        # directly.\n",
    "        data_file = self.training_file if self.train else self.test_file\n",
    "        return torch.load(os.path.join(self.processed_folder, data_file))\n",
    "        \n",
    "    def _load_data(self):\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        data = read_image_file(os.path.join(self.raw_folder, image_file))\n",
    "        \n",
    "        # Compute MURA encoder and decoder (numpy)\n",
    "        self.A = mura.create_binary_aperture_arr(self.image_size)\n",
    "        self.G = mura.create_decoding_arr(self.A)\n",
    "        # Resize data to prime number length and convolve with aperture\n",
    "        data_resized = torchvision.transforms.functional.resize(data, [self.image_size,self.image_size], antialias=True)\n",
    "        mura_data = torch.empty(data_resized.size())\n",
    "        for idx, img in enumerate(data_resized):\n",
    "            mura_data[idx] = torch.tensor(\n",
    "                            mura.normalize(mura.FFT_convolve(\n",
    "                            np.squeeze(img.numpy()), self.A,self.image_size)), dtype= torch.float)\n",
    "            \n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        digits = read_label_file(os.path.join(self.raw_folder, label_file))\n",
    "        \n",
    "        \n",
    "        # #Technically, we do not even need the labels for now\n",
    "        # # We just need the clean images of both types\n",
    "        # randata = data[torch.randperm(data.shape[0]),:,:]\n",
    "        # targets = (data, randata)\n",
    "        \n",
    "        # # Now do the ambiguation here\n",
    "        # data = data + randata\n",
    "        \n",
    "        return mura_data, data_resized, digits\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target, digit = self.data[index], self.targets[index], self.digits[index]\n",
    "        #Change img to numpy and range to [0,155]\n",
    "        img = np.uint8((img*255).numpy())\n",
    "        \n",
    "        #doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Imagedata[torch.randperm(data.shape[0]),:,:]\n",
    "        img = Image.fromarray(img, mode='L')\n",
    "        target = Image.fromarray(target.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)            \n",
    "\n",
    "        return dict({'img': img, 'target': target, 'digit': digit.item()})\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "        \n",
    "    def _read_params(self, params):\n",
    "        self.image_size = self.params['image_size']\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self):\n",
    "        return os.path.join(self.root, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self):\n",
    "        return os.path.join(self.root, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c063d5-6039-4ab2-a865-b7ae6655c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create params dict\n",
    "params = {\n",
    "  \"image_size\": 23,\n",
    "  \"batch_size\":100,\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19e058d3-a669-4e51-b36c-99f324565141",
   "metadata": {},
   "source": [
    "# Load MNIST and define dataloader\n",
    "mnist_train = torchvision.datasets.MNIST('../data/MNIST/',download=False)\n",
    "train_data_loader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                          batch_size=params['batch_size'],\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c13e1b7a-0a08-4db8-8c84-e340fa4081b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform_list = transforms.Compose(\n",
    "            [   transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(params['image_size'], antialias=True),\n",
    "                Noise_dB(desired_snr=1),\n",
    "                # transforms.Normalize(0, 1),\n",
    "                ])\n",
    "target_transform = transforms.Compose(\n",
    "            [   transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((params['image_size'],params['image_size']), antialias=True),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a6e757d-5b6e-427b-a406-715d55290b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mura = MNIST_MURA('../data/MNIST/', params, transform=transform_list, target_transform=target_transform, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb860400-0d5d-4fd8-814c-e717ab5e7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DataLoader(mnist_mura, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "900cc389-487e-4fab-a03e-847a46dbf3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,7))\n",
    "idx=1\n",
    "axs[0].imshow(trainset.dataset[idx]['target'].squeeze(0), cmap= 'gray')\n",
    "axs[1].imshow(trainset.dataset[idx]['img'].squeeze(0), cmap= 'gray')\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53f2ee-44bd-4698-90f9-54281a4d4707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
